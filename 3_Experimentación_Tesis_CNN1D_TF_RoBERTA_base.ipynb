{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebTejfKHEXW2"
      },
      "source": [
        "# Clasificación de metafora por medio de CNN utilizando el modelo Roberta-large:\n",
        "\n",
        "A partir de este modelo preentrenado se generan los embeddings correspondientes a cada oración del corpus Trofi, una vez generados los vectores se ajustan a una dimensión de (1, 1024)  y se propone una red convolucional (1 dimensión) de una capa conectada a una capa densamente conectada como clasificador.\n",
        "\n",
        "Las etiquetas de cada oración se presentan en formato one hot:\n",
        "\n",
        "0 --> [1, 0] --> Texto literal\n",
        "\n",
        "1 --> [0, 1] --> Texto metaforico\n",
        "\n",
        "Es importante aclarar que el funcionamiento de la red se probó con hasta 6 capas convolucionales pero su desempeño es mejor con una sola capa convolucional.\n",
        "\n",
        "\n",
        "*Proyecto de tésis 2022, para el grado de Maestría en Ciencias en Ingenieria del Computo.\n",
        "CIC-IPN*\n",
        "\n",
        "*Alumna: Ericka Deyanira Ovando B.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBYFW6QXsIHt",
        "outputId": "0f39ce51-c2a1-4e24-d483-2c2c29ce8372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive #para importar y exportar archivos desde drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa1Rd_E7yaeQ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf #se importan las librerias a utilizar\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "#%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONrHFbtFliKO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "data = pickle.load(open('/content/drive/MyDrive/SEROBERTABASE',  'rb'))\n",
        "x_train_total = data[0]\n",
        "y_train_onehot_total= data[1]\n",
        "sentence_train_total= data[2]\n",
        "x_test_total= data[3]\n",
        "y_test_onehot_total= data[4]\n",
        "sentence_test_total= data[5]\n",
        "#se importan los datos almacenados en pickle correspondientes a los conjuntos de\n",
        "#k- fold- cross- validation de senteces embeddings generados a partir de Roberta-large del dataset Trofi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sZPwZBznhpH"
      },
      "outputs": [],
      "source": [
        "x_train= x_train_total[0]\n",
        "y_train_onehot=y_train_onehot_total[0]\n",
        "sentence_train=sentence_train_total[0]\n",
        "x_test= x_test_total[0]\n",
        "y_test_onehot= y_test_onehot_total[0]\n",
        "sentence_test= sentence_test_total[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRqfmVACWFLv",
        "outputId": "85420823-e415-4ddf-f541-46b0d68d4be0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "x_train_total[0][1].shape #forma de x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JobIrYGYv_pR",
        "outputId": "4b297353-c7cd-4ff0-a94b-2d7f2963ca21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "y_test_onehot_total[0][0] #forma de las etiquetas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju85XQl8H6Nb"
      },
      "source": [
        "# Para utilizar la GPU disponible en colaboratory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFlCfgyPE95E",
        "outputId": "f54644ab-878b-4d0c-ebe7-9371b3fc44fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 24 07:41:59 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P0    29W /  70W |    438MiB / 15109MiB |      1%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aSAy6EsILQU"
      },
      "source": [
        "# Se diseña la red convolucional utilizando TF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "737wxINOnrhw",
        "outputId": "09129e4d-d551-44e1-ba70-cde30d15b759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "#Cargamos la versión de TF a usar\n",
        "\n",
        "import tensorflow as tf  #Se revisa si está disponible GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0EzYI2inrPh"
      },
      "outputs": [],
      "source": [
        "#Declaramos la arquitectura de la red...\n",
        "class DNN_model(object):\n",
        "  def __init__(self,\n",
        "               n_classes=2):\n",
        "    #Declaramos los pesos y bías de cada capa\n",
        "    #[128, 128, 3]-> [64,64, 64]->[32,32,128 ]->[16,16,128]\n",
        "    self.h1LW = tf.Variable(np.random.rand(5,1,32),name=\"hl1weigths\",dtype=\"float32\") #capa conv 1\n",
        "    self.h1LB = tf.Variable(np.random.rand(32),name=\"hl1bias\",dtype=\"float32\")\n",
        "\n",
        "    self.h2LW = tf.Variable(np.random.rand(5,32,64),name=\"hl2weigths\",dtype=\"float32\")  #capa coonv 2\n",
        "    self.h2LB = tf.Variable(np.random.rand(64),name=\"hl2bias\",dtype=\"float32\")\n",
        "\n",
        "    self.h3LW = tf.Variable(np.random.rand(5,64,128),name=\"hl2weigths_O\",dtype=\"float32\") #capa conv 3\n",
        "    self.h3LB = tf.Variable(np.random.rand(128),name=\"hl2bias_O\",dtype=\"float32\")\n",
        "\n",
        "    self.h4LW = tf.Variable(np.random.rand(5,128,256),name=\"hl2weigths_O\",dtype=\"float32\") #capa conv 3\n",
        "    self.h4LB = tf.Variable(np.random.rand(256),name=\"hl2bias_O\",dtype=\"float32\")\n",
        "\n",
        "    self.h5LW = tf.Variable(np.random.rand(5,256,512),name=\"hl2weigths_O\",dtype=\"float32\") #capa conv 3\n",
        "    self.h5LB = tf.Variable(np.random.rand(512),name=\"hl2bias_O\",dtype=\"float32\")\n",
        "\n",
        "    self.h6LW = tf.Variable(np.random.rand(5,512,512),name=\"hl2weigths_O\",dtype=\"float32\") #capa conv 3\n",
        "    self.h6LB = tf.Variable(np.random.rand(512),name=\"hl2bias_O\",dtype=\"float32\")\n",
        "\n",
        "    self.h7LW = tf.Variable(np.random.rand(1*96*128,1024),name=\"hl3weigths\",dtype=\"float32\") #capa aplanar vector 4\n",
        "    self.h7LB = tf.Variable(np.random.rand(1024),name=\"hl3bias\",dtype=\"float32\")\n",
        "\n",
        "    self.outW = tf.Variable(np.random.rand(1024, n_classes),name=\"outweigths\",dtype=\"float32\") #capa conectada 5\n",
        "\n",
        "    self.outB = tf.Variable(np.random.rand(n_classes),name=\"outbias\",dtype=\"float32\")\n",
        "    self.trainable_variables =[self.h1LW,self.h1LB,\n",
        "                               self.h2LW,self.h2LB,\n",
        "                               self.h3LW,self.h3LB,\n",
        "                               self.h4LW,self.h4LB,\n",
        "                               self.h5LW,self.h5LB,\n",
        "                               self.h6LW,self.h6LB,\n",
        "                               self.h7LW,self.h7LB,\n",
        "                               self.outW,self.outB]\n",
        "\n",
        "  def __call__(self,x, rate):\n",
        "\n",
        "      x  = tf.cast(x, tf.float32)\n",
        "      print('x', x.shape)\n",
        "      #El formato es N -> datos en el batch\n",
        "      # H -> height\n",
        "      # W -> width\n",
        "      # C -> channels\n",
        "      # [N, H, W, C]\n",
        "\n",
        "      img = tf.reshape(x, shape=[-1, 1, 768, 1])\n",
        "      print('img:', img.shape)\n",
        "\n",
        "      #primera capa conv ---> [1, 1024]\n",
        "      l1 = tf.add(img, self.h1LB)\n",
        "      l1= tf.nn.conv1d(l1,self.h1LW, stride= 1, padding='SAME')\n",
        "      mean_1,std_1 = tf.nn.moments(l1, axes=[0,1,2])\n",
        "      l1= tf.nn.batch_normalization(l1, mean_1, std_1, None, None, 1e-12)\n",
        "      l1 = tf.nn.relu(l1)\n",
        "\n",
        "      l1 = tf.nn.max_pool(l1, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "      print('l1', l1.shape)\n",
        "\n",
        "      #segunda capa conv ---> [1, 512]\n",
        "      l2= tf.nn.conv1d(l1,self.h2LW, stride= 1, padding='SAME')\n",
        "      l2 = tf.add(l2, self.h2LB)\n",
        "      mean_2,std_2 = tf.nn.moments(l2, axes=[0,1,2])\n",
        "      l2=tf.nn.batch_normalization(l2, mean_2, std_2, None, None, 1e-12)\n",
        "      l2 = tf.nn.relu(l2)\n",
        "      l2 = tf.nn.max_pool(l2, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "      print('l2', l2.shape)\n",
        "\n",
        "      #tercera capa conv 3 ---> [1, 256]\n",
        "      l3= tf.nn.conv1d(l2,self.h3LW, stride= 1, padding='SAME')\n",
        "      l3 = tf.add(l3, self.h3LB)\n",
        "      mean_3,std_3 = tf.nn.moments(l3, axes=[0,1,2])\n",
        "      l3 =tf.nn.batch_normalization(l3, mean_3, std_3, None, None, 1e-12)\n",
        "      l3 = tf.nn.relu(l3)\n",
        "      l3 = tf.nn.max_pool(l3, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "      print('l3', l3.shape)\n",
        "\n",
        "      #cuarta capa conv 4 ---> [1, 128]\n",
        "      l4= tf.nn.conv1d(l3,self.h4LW, stride= 1 , padding='SAME')\n",
        "      l4 = tf.add(l4, self.h4LB)\n",
        "      mean_4,std_4 = tf.nn.moments(l4, axes=[0,1,2])\n",
        "      l4 =tf.nn.batch_normalization(l4, mean_4, std_4, None, None, 1e-12)\n",
        "      l4 = tf.nn.relu(l4)\n",
        "      l4 = tf.nn.max_pool(l4, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "      print(l4.shape, 'l4')\n",
        "\n",
        "      #quinta capa conv 4 ---> [1, 64]\n",
        "      l5= tf.nn.conv1d(l4,self.h5LW, stride= 1, padding='SAME')\n",
        "      l5 = tf.add(l5, self.h5LB)\n",
        "      mean_5,std_5 = tf.nn.moments(l5, axes=[0,1,2])\n",
        "      l5 =tf.nn.batch_normalization(l5, mean_5, std_5, None, None, 1e-12)\n",
        "      l5 = tf.nn.relu(l5)\n",
        "      l5 = tf.nn.max_pool(l5, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "      print(l5.shape, 'l5')\n",
        "\n",
        "      #sexta capa conv 4 ---> [1, 32]\n",
        "      l6= tf.nn.conv1d(l5,self.h6LW, stride= 1, padding='SAME')\n",
        "      l6 = tf.add(l6, self.h6LB)\n",
        "      mean_6,std_6 = tf.nn.moments(l6, axes=[0,1,2])\n",
        "      l6 =tf.nn.batch_normalization(l6, mean_6, std_6, None, None, 1e-12)\n",
        "      l6 = tf.nn.relu(l6)\n",
        "      l6 = tf.nn.max_pool(l6, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
        "      print(l6.shape, 'l6')\n",
        "      print( 'input shape:', l6.shape)\n",
        "\n",
        "      # sexta capa para aplanar vector  -> 1* 64* 128\n",
        "      l7=  tf.reshape(l2,[-1, 1*96*128])\n",
        "      l7=  tf.matmul(l7,self.h7LW)\n",
        "      l7= tf.add(l7, self.h7LB)\n",
        "      print(l7.shape)\n",
        "\n",
        "      l7 = tf.nn.relu(l7)\n",
        "      l7=tf.nn.dropout(l7, rate)\n",
        "\n",
        "      output = tf.matmul(l7,self.outW) + self.outB\n",
        "      print('output', len(output))\n",
        "      print(output.shape)\n",
        "      return output\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27bAGsF1nrEG",
        "outputId": "d04ecd48-4e0c-49f2-ed21-300eb8817e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (10, 1, 768)\n",
            "img: (10, 1, 768, 1)\n",
            "l1 (10, 1, 384, 32)\n",
            "l2 (10, 1, 192, 64)\n",
            "l3 (10, 1, 96, 128)\n",
            "(10, 1, 48, 256) l4\n",
            "(10, 1, 24, 512) l5\n",
            "(10, 1, 12, 512) l6\n",
            "input shape: (10, 1, 12, 512)\n",
            "(10, 1024)\n",
            "output 10\n",
            "(10, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
              "array([[489258.25, 498035.4 ],\n",
              "       [467475.3 , 448858.62],\n",
              "       [552937.8 , 552084.56],\n",
              "       [580223.7 , 548106.6 ],\n",
              "       [512560.56, 487258.66],\n",
              "       [474876.6 , 474440.34],\n",
              "       [606512.4 , 560655.94],\n",
              "       [532882.3 , 501608.12],\n",
              "       [504580.28, 484304.6 ],\n",
              "       [649120.5 , 633171.06]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "DNN = DNN_model()\n",
        "DNN(x_train[1600: 1610], 0.6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWpnQBGCoQ6M"
      },
      "outputs": [],
      "source": [
        "from os import listdir # se cargan las librerias necesarias\n",
        "from os.path import isfile, join\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime, os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z1VRtWIoQq0"
      },
      "outputs": [],
      "source": [
        "#Cargamos un oprtimizador, como siempre, usaremos el de cajón con un learning rate comun\n",
        "optimizador = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\n",
        "#optimizador= tf. compat. v1.train.AdagradOptimizer(learning_rate=0.001)\n",
        "#optimizador= tf.compat.v1.train.AdadeltaOptimizer(learning_rate=.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTr1OKJpoQfx"
      },
      "outputs": [],
      "source": [
        "#Declaramos las metricas, para la perdida y el accuracy...\n",
        "#Para training\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "#Para testing\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmjfROYnoPZc"
      },
      "outputs": [],
      "source": [
        "#Hacemos nuestra fnción del paso de entrenamiento...\n",
        "@tf.function\n",
        "def train_step(model,tdata, labels, rate):\n",
        "  with tf.GradientTape() as tape:\n",
        "    #Se propagan los datos y se obtiene una salida...\n",
        "    predictions = model(tdata, rate)\n",
        "\n",
        "    #calculo de una funcion de error\n",
        "    #cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "    #loss= tf.reduce_mean(cce(labels, predictions))\n",
        "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
        "\n",
        "  #Calculamos  los gradientes\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  #ordenamos los parametros con su gradiente...\n",
        "  capped_grads_and_vars = [(grad,model.trainable_variables[index]) for index, grad in enumerate(gradients)]\n",
        "  #Aplicamos nuestro optimizador seleccionado\n",
        "  optimizador.apply_gradients(capped_grads_and_vars)\n",
        "  #perdida\n",
        "  train_loss(loss)\n",
        "  #accuracy...\n",
        "  train_accuracy(labels, predictions)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZHJb-cOuapg"
      },
      "outputs": [],
      "source": [
        "#Nuestro paso de prueba...\n",
        "#Nuestro paso de prueba...\n",
        "@tf.function\n",
        "def test_step(model,tdata, labels):\n",
        "  #Hacemos la propagacion de los datos y obtenemos la salida\n",
        "  predictions = model(tdata, 0) #para probar el rate=0\n",
        "  #Calulamos el error\n",
        "  #t_cce = tf.keras.losses.CategoricalCrossentropy()\n",
        "  #t_loss= tf.reduce_mean(t_cce(labels, predictions))\n",
        "  t_loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels, predictions))\n",
        "\n",
        "  # error\n",
        "  test_loss(t_loss)\n",
        "  # accuracy\n",
        "  test_accuracy(labels, predictions)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_step(DNN,x_test[0:10],y_test_onehot[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b90WAHszP2t7",
        "outputId": "456f5a7c-f126-4cd4-8212-403c43358da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (10, 1, 768)\n",
            "img: (10, 1, 768, 1)\n",
            "l1 (10, 1, 384, 32)\n",
            "l2 (10, 1, 192, 64)\n",
            "l3 (10, 1, 96, 128)\n",
            "(10, 1, 48, 256) l4\n",
            "(10, 1, 24, 512) l5\n",
            "(10, 1, 12, 512) l6\n",
            "input shape: (10, 1, 12, 512)\n",
            "(10, 1024)\n",
            "output 10\n",
            "(10, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmMz2Y_WuaNC"
      },
      "outputs": [],
      "source": [
        "#Aqui agregamos el registro de todo lo que queremos almacenar\n",
        "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") #Un marcador del tiempo actual\n",
        "\n",
        "train_log_dir = '/content/drive/MyDrive/logs/gradient_tape/' + current_time + '/train' #registro de entrenamiento\n",
        "test_log_dir = '/content/drive/MyDrive/logs/gradient_tape/' + current_time + '/test' #Hregistro de pruebas\n",
        "train_summary_writer = tf.summary.create_file_writer(train_log_dir) #escriba de entrenamiento\n",
        "test_summary_writer = tf.summary.create_file_writer(test_log_dir) # escriba de pruebas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjNzclJUvOOL"
      },
      "outputs": [],
      "source": [
        "#Declaramos la funcion de entrenamiento\n",
        "def fitting(model,train_x,train_y,test_x,test_y,EPOCHS,N_batch,batch_size, rate):\n",
        "  \"\"\"\n",
        "  Esta función lleva a cabo el entrenamiento de una red neuronal\n",
        "  Recibe:\n",
        "  * model.- La red neuronal a entrenar. Debe ser un objeto de la clase \"DNN_model\"\n",
        "  * train_x.- El conjunto de datos de entrenamiento.\n",
        "  * train_y.- El conjutno de etiquetas de entrenamiento.\n",
        "  * test_x.- El conjunto de datos de prueba.\n",
        "  * test_y.- El conjunto de etiquetas de prueba.\n",
        "  * EPOCHS.- El número de épocas de entrenamiento.\n",
        "  * N_batch.- El número de lotes (batches) a procesar\n",
        "  * bacth_size.- El tamaño de cada lote (batch)\n",
        "\n",
        "  No entrega nada como salida pero muestra en pantalla el proceso del entrenamiento\n",
        "  \"\"\"\n",
        "  #El ciclo de numero de epocas\n",
        "  img = np.reshape(train_x[0], (-1, 1, 768, 1))\n",
        "  with train_summary_writer.as_default():\n",
        "\n",
        "    tf.summary.image(\"Training data\", img, step=0)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    i=0\n",
        "    #El ciclo por los batches...\n",
        "    while i+batch_size < len(train_x): #or i+batch_size<batch_size*N_batch:\n",
        "      #Calculamos el inicio y fin del lote...\n",
        "      start = i\n",
        "      end = i+batch_size\n",
        "      #Tomamos la rebanada en train_x y train_y a propagar por la red...\n",
        "      batch_x = train_x[start:end]\n",
        "      batch_y = train_y[start:end]\n",
        "      #Hacemos el paso de entrenamiento, enviamos la red, los datos y sus etiquetas...\n",
        "      train_step(model,batch_x,batch_y, rate)\n",
        "      #Hacemos el incremento en el contador del lote...\n",
        "      i+=batch_size\n",
        "    #Hacemos el paso de prueba...\n",
        "    with train_summary_writer.as_default():\n",
        "      #tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "      tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "    tf.summary.trace_on(graph=True, profiler=True)\n",
        "    test_step(model,test_x,test_y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.trace_export(\n",
        "        name=\"function_trace\", step=0, profiler_outdir= train_log_dir)\n",
        "\n",
        "    with test_summary_writer.as_default():\n",
        "      tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "      tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "      #tf.summary.scalar('fp', test_fp.result(), step=epoch)\n",
        "      #tf.summary.scalar('fn', test_fn.result(), step=epoch)\n",
        "      #tf.summary.scalar('tp', test_tp.result(), step=epoch)\n",
        "      #tf.summary.scalar('tn', test_tn.result(), step=epoch)\n",
        "      #tf.summary.scalar('precisión', test_precision.result(), step=epoch)\n",
        "      #tf.summary.scalar('recall', test_recall.result(), step=epoch)\n",
        "\n",
        "\n",
        "    #Desplegamos en pantalla la epoca, la pérdida y el error de entrenamient y de prueba...\n",
        "    template = 'Epoch {}, Perdida: {}, Exactitud: {}, Perdida de prueba: {}, Exactitud de prueba: {}' #,Fp {}, Fn {}, Tp {}, Tn {}, presición {}, recall {}'\n",
        "    print(template.format(epoch+1,\n",
        "                        train_loss.result(),\n",
        "                        train_accuracy.result(),\n",
        "                        test_loss.result(),\n",
        "                        test_accuracy.result(),\n",
        "                        #test_fp.result(),\n",
        "                        #test_fn.result(),\n",
        "                        #test_tp.result(),\n",
        "                        #test_tn.result(),\n",
        "                        #test_precision.result(),\n",
        "                        #test_recall.result(),\n",
        "                        ))\n",
        "\n",
        "    #Reseteamos las metricas para que no se acumule la información de todas las épocas...\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    #test_fp.reset_states()\n",
        "    #test_fn.reset_states()\n",
        "    #test_tp.reset_states()\n",
        "    #test_tn.reset_states()\n",
        "    #test_precision.reset_states()\n",
        "    #test_recall.reset_states()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFMIL25zJDI7"
      },
      "source": [
        "# Se entrena la red y se prueba con los diferentes conjuntos k- fold validation para evaluar su desempeño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMqC6vW0grlZ"
      },
      "outputs": [],
      "source": [
        "#Declaramos la funcion de entrenamiento\n",
        "def fitting2(model,train_x,train_y,test_x,test_y,EPOCHS,N_batch,batch_size, rate):\n",
        "  \"\"\"\n",
        "  Esta función lleva a cabo el entrenamiento de una red neuronal\n",
        "  Recibe:\n",
        "  * model.- La red neuronal a entrenar. Debe ser un objeto de la clase \"DNN_model\"\n",
        "  * train_x.- El conjunto de datos de entrenamiento.\n",
        "  * train_y.- El conjutno de etiquetas de entrenamiento.\n",
        "  * test_x.- El conjunto de datos de prueba.\n",
        "  * test_y.- El conjunto de etiquetas de prueba.\n",
        "  * EPOCHS.- El número de épocas de entrenamiento.\n",
        "  * N_batch.- El número de lotes (batches) a procesar\n",
        "  * bacth_size.- El tamaño de cada lote (batch)\n",
        "\n",
        "  No entrega nada como salida pero muestra en pantalla el proceso del entrenamiento\n",
        "  \"\"\"\n",
        "  #El ciclo de numero de epocas\n",
        "  img = np.reshape(train_x[0], (-1, 1, 512, 1))\n",
        "  with train_summary_writer.as_default():\n",
        "\n",
        "    tf.summary.image(\"Training data\", img, step=0)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    i=0\n",
        "    #El ciclo por los batches...\n",
        "    while i+batch_size < len(train_x): #or i+batch_size<batch_size*N_batch:\n",
        "      #Calculamos el inicio y fin del lote...\n",
        "      start = i\n",
        "      end = i+batch_size\n",
        "      #Tomamos la rebanada en train_x y train_y a propagar por la red...\n",
        "      batch_x = train_x[start:end]\n",
        "      batch_y = train_y[start:end]\n",
        "      #Hacemos el paso de entrenamiento, enviamos la red, los datos y sus etiquetas...\n",
        "      train_step(model,batch_x,batch_y, rate)\n",
        "      #Hacemos el incremento en el contador del lote...\n",
        "      i+=batch_size\n",
        "    #Hacemos el paso de prueba...\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "      tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "    tf.summary.trace_on(graph=True, profiler=True)\n",
        "    test_step(model,test_x,test_y)\n",
        "    print('x', test_x[1], 'y', test_y[1])\n",
        "\n",
        "\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.trace_export(\n",
        "        name=\"function_trace\", step=0, profiler_outdir= train_log_dir)\n",
        "\n",
        "    with test_summary_writer.as_default():\n",
        "      tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "      tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "\n",
        "    #Desplegamos en pantalla la epoca, la pérdida y el error de entrenamient y de prueba...\n",
        "    template = '{}'\n",
        "    print(template.format(\n",
        "                        test_accuracy.result()))\n",
        "\n",
        "    #Reseteamos las metricas para que no se acumule la información de todas las épocas...\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    test_fn.reset_state()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gy1wJ_ivOKy",
        "outputId": "872750d7-5389-4230-d31e-eb4eefc717e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (200, 1, 768)\n",
            "img: (200, 1, 768, 1)\n",
            "l1 (200, 1, 384, 32)\n",
            "l2 (200, 1, 192, 64)\n",
            "l3 (200, 1, 96, 128)\n",
            "(200, 1, 48, 256) l4\n",
            "(200, 1, 24, 512) l5\n",
            "(200, 1, 12, 512) l6\n",
            "input shape: (200, 1, 12, 512)\n",
            "(200, 1024)\n",
            "output 200\n",
            "(200, 2)\n",
            "x (200, 1, 768)\n",
            "img: (200, 1, 768, 1)\n",
            "l1 (200, 1, 384, 32)\n",
            "l2 (200, 1, 192, 64)\n",
            "l3 (200, 1, 96, 128)\n",
            "(200, 1, 48, 256) l4\n",
            "(200, 1, 24, 512) l5\n",
            "(200, 1, 12, 512) l6\n",
            "input shape: (200, 1, 12, 512)\n",
            "(200, 1024)\n",
            "output 200\n",
            "(200, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1332: start (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.start` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1383: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1383: save (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/profiler.py:150: maybe_create_event_file (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "`tf.python.eager.profiler` has deprecated, use `tf.profiler` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Perdida: 30438.265625, Exactitud: 0.5803124904632568, Perdida de prueba: 13944.4853515625, Exactitud de prueba: 0.582245409488678\n",
            "Epoch 2, Perdida: 35195.05859375, Exactitud: 0.3634375035762787, Perdida de prueba: 594.177978515625, Exactitud de prueba: 0.5764074921607971\n",
            "Epoch 3, Perdida: 10145.0244140625, Exactitud: 0.47593748569488525, Perdida de prueba: 13520.5419921875, Exactitud de prueba: 0.5710455775260925\n",
            "Epoch 4, Perdida: 19305.224609375, Exactitud: 0.21250000596046448, Perdida de prueba: 3932.623291015625, Exactitud de prueba: 0.5710455775260925\n",
            "Epoch 5, Perdida: 10499.7216796875, Exactitud: 0.32624998688697815, Perdida de prueba: 6847.58154296875, Exactitud de prueba: 0.5710455775260925\n",
            "Epoch 6, Perdida: 11880.712890625, Exactitud: 0.2809374928474426, Perdida de prueba: 3857.843505859375, Exactitud de prueba: 0.5764074921607971\n",
            "Epoch 7, Perdida: 9222.736328125, Exactitud: 0.32249999046325684, Perdida de prueba: 4251.5009765625, Exactitud de prueba: 0.5764074921607971\n",
            "Epoch 8, Perdida: 8722.9599609375, Exactitud: 0.3165625035762787, Perdida de prueba: 3630.197021484375, Exactitud de prueba: 0.5764074921607971\n",
            "Epoch 9, Perdida: 8431.150390625, Exactitud: 0.3187499940395355, Perdida de prueba: 3284.77783203125, Exactitud de prueba: 0.5790884494781494\n",
            "Epoch 10, Perdida: 7155.36181640625, Exactitud: 0.35624998807907104, Perdida de prueba: 2700.482666015625, Exactitud de prueba: 0.5844504237174988\n",
            "Epoch 11, Perdida: 6479.1396484375, Exactitud: 0.37281250953674316, Perdida de prueba: 2826.800048828125, Exactitud de prueba: 0.5844504237174988\n",
            "Epoch 12, Perdida: 6289.47412109375, Exactitud: 0.36656248569488525, Perdida de prueba: 2264.3974609375, Exactitud de prueba: 0.6005361676216125\n",
            "Epoch 13, Perdida: 5456.4755859375, Exactitud: 0.39531248807907104, Perdida de prueba: 2241.489990234375, Exactitud de prueba: 0.6032171845436096\n",
            "Epoch 14, Perdida: 5361.62109375, Exactitud: 0.3903124928474426, Perdida de prueba: 1821.2559814453125, Exactitud de prueba: 0.6219838857650757\n",
            "Epoch 15, Perdida: 4763.81494140625, Exactitud: 0.4009374976158142, Perdida de prueba: 1637.04931640625, Exactitud de prueba: 0.6541554927825928\n",
            "Epoch 16, Perdida: 4488.07568359375, Exactitud: 0.42093750834465027, Perdida de prueba: 1432.7265625, Exactitud de prueba: 0.6729222536087036\n",
            "Epoch 17, Perdida: 3955.923583984375, Exactitud: 0.44999998807907104, Perdida de prueba: 1567.7406005859375, Exactitud de prueba: 0.6541554927825928\n",
            "Epoch 18, Perdida: 3945.820556640625, Exactitud: 0.43312498927116394, Perdida de prueba: 1357.8935546875, Exactitud de prueba: 0.6782841682434082\n",
            "Epoch 19, Perdida: 3650.368408203125, Exactitud: 0.44593751430511475, Perdida de prueba: 1264.609130859375, Exactitud de prueba: 0.6863270998001099\n",
            "Epoch 20, Perdida: 3360.63037109375, Exactitud: 0.4612500071525574, Perdida de prueba: 1284.2208251953125, Exactitud de prueba: 0.6809651255607605\n",
            "Epoch 21, Perdida: 3343.547607421875, Exactitud: 0.45625001192092896, Perdida de prueba: 1151.2401123046875, Exactitud de prueba: 0.6943699717521667\n",
            "Epoch 22, Perdida: 3148.72900390625, Exactitud: 0.46562498807907104, Perdida de prueba: 1072.29931640625, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 23, Perdida: 2715.27734375, Exactitud: 0.49281251430511475, Perdida de prueba: 1109.5426025390625, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 24, Perdida: 2929.558837890625, Exactitud: 0.4515624940395355, Perdida de prueba: 1071.1446533203125, Exactitud de prueba: 0.6916890144348145\n",
            "Epoch 25, Perdida: 2811.47216796875, Exactitud: 0.46000000834465027, Perdida de prueba: 882.540771484375, Exactitud de prueba: 0.7104557752609253\n",
            "Epoch 26, Perdida: 2493.228515625, Exactitud: 0.4803124964237213, Perdida de prueba: 997.4029541015625, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 27, Perdida: 2483.888671875, Exactitud: 0.4724999964237213, Perdida de prueba: 887.4503173828125, Exactitud de prueba: 0.6997318863868713\n",
            "Epoch 28, Perdida: 2474.202880859375, Exactitud: 0.46968749165534973, Perdida de prueba: 821.6412353515625, Exactitud de prueba: 0.707774817943573\n",
            "Epoch 29, Perdida: 2226.438720703125, Exactitud: 0.4884375035762787, Perdida de prueba: 828.0470581054688, Exactitud de prueba: 0.6997318863868713\n",
            "Epoch 30, Perdida: 2112.6357421875, Exactitud: 0.48500001430511475, Perdida de prueba: 729.2512817382812, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 31, Perdida: 1997.5810546875, Exactitud: 0.49437499046325684, Perdida de prueba: 784.6930541992188, Exactitud de prueba: 0.7104557752609253\n",
            "Epoch 32, Perdida: 2046.63720703125, Exactitud: 0.4637500047683716, Perdida de prueba: 718.0650024414062, Exactitud de prueba: 0.7104557752609253\n",
            "Epoch 33, Perdida: 1854.193115234375, Exactitud: 0.4946874976158142, Perdida de prueba: 667.9055786132812, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 34, Perdida: 1678.080078125, Exactitud: 0.5071874856948853, Perdida de prueba: 720.6524658203125, Exactitud de prueba: 0.7131367325782776\n",
            "Epoch 35, Perdida: 1743.03271484375, Exactitud: 0.48124998807907104, Perdida de prueba: 618.9140014648438, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 36, Perdida: 1645.46337890625, Exactitud: 0.4871875047683716, Perdida de prueba: 592.4866943359375, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 37, Perdida: 1511.6248779296875, Exactitud: 0.5118749737739563, Perdida de prueba: 576.3587646484375, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 38, Perdida: 1370.973388671875, Exactitud: 0.504687488079071, Perdida de prueba: 598.5419311523438, Exactitud de prueba: 0.7131367325782776\n",
            "Epoch 39, Perdida: 1423.9805908203125, Exactitud: 0.4962500035762787, Perdida de prueba: 505.7370910644531, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 40, Perdida: 1232.90185546875, Exactitud: 0.5140625238418579, Perdida de prueba: 522.2416381835938, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 41, Perdida: 1211.1165771484375, Exactitud: 0.5199999809265137, Perdida de prueba: 483.72613525390625, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 42, Perdida: 1134.5390625, Exactitud: 0.5228124856948853, Perdida de prueba: 458.87298583984375, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 43, Perdida: 1084.2109375, Exactitud: 0.53125, Perdida de prueba: 462.65570068359375, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 44, Perdida: 1113.3499755859375, Exactitud: 0.4937500059604645, Perdida de prueba: 416.71221923828125, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 45, Perdida: 1051.5556640625, Exactitud: 0.5134375095367432, Perdida de prueba: 379.9207458496094, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 46, Perdida: 884.5087890625, Exactitud: 0.5443750023841858, Perdida de prueba: 409.2452697753906, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 47, Perdida: 937.0337524414062, Exactitud: 0.504687488079071, Perdida de prueba: 352.373291015625, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 48, Perdida: 797.4617919921875, Exactitud: 0.5462499856948853, Perdida de prueba: 352.7810363769531, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 49, Perdida: 772.5939331054688, Exactitud: 0.5415624976158142, Perdida de prueba: 347.7096862792969, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 50, Perdida: 754.042236328125, Exactitud: 0.5212500095367432, Perdida de prueba: 328.4933776855469, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 51, Perdida: 745.7044677734375, Exactitud: 0.5181249976158142, Perdida de prueba: 283.578369140625, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 52, Perdida: 654.7611694335938, Exactitud: 0.5340625047683716, Perdida de prueba: 300.9251708984375, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 53, Perdida: 648.2365112304688, Exactitud: 0.5237500071525574, Perdida de prueba: 277.1747131347656, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 54, Perdida: 601.2001953125, Exactitud: 0.5299999713897705, Perdida de prueba: 264.0610656738281, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 55, Perdida: 565.01123046875, Exactitud: 0.5196874737739563, Perdida de prueba: 252.69105529785156, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 56, Perdida: 549.4732666015625, Exactitud: 0.5253124833106995, Perdida de prueba: 229.00595092773438, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 57, Perdida: 496.0776672363281, Exactitud: 0.5425000190734863, Perdida de prueba: 223.68447875976562, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 58, Perdida: 483.48114013671875, Exactitud: 0.5362499952316284, Perdida de prueba: 206.4766845703125, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 59, Perdida: 453.66162109375, Exactitud: 0.5403125286102295, Perdida de prueba: 195.89561462402344, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 60, Perdida: 413.018310546875, Exactitud: 0.5490624904632568, Perdida de prueba: 193.4169158935547, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 61, Perdida: 413.0857238769531, Exactitud: 0.5271875262260437, Perdida de prueba: 187.4654541015625, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 62, Perdida: 388.0113525390625, Exactitud: 0.5309374928474426, Perdida de prueba: 166.25265502929688, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 63, Perdida: 353.013671875, Exactitud: 0.534375011920929, Perdida de prueba: 156.991943359375, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 64, Perdida: 324.7889099121094, Exactitud: 0.542187511920929, Perdida de prueba: 156.70921325683594, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 65, Perdida: 311.8246765136719, Exactitud: 0.5425000190734863, Perdida de prueba: 145.1571807861328, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 66, Perdida: 294.0687561035156, Exactitud: 0.5293750166893005, Perdida de prueba: 145.28436279296875, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 67, Perdida: 293.73394775390625, Exactitud: 0.5143749713897705, Perdida de prueba: 131.22276306152344, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 68, Perdida: 263.97735595703125, Exactitud: 0.5346875190734863, Perdida de prueba: 118.2440185546875, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 69, Perdida: 233.57791137695312, Exactitud: 0.5337499976158142, Perdida de prueba: 121.76164245605469, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 70, Perdida: 226.33050537109375, Exactitud: 0.5159375071525574, Perdida de prueba: 107.20709228515625, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 71, Perdida: 213.0240936279297, Exactitud: 0.5278124809265137, Perdida de prueba: 98.92959594726562, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 72, Perdida: 191.7740020751953, Exactitud: 0.5337499976158142, Perdida de prueba: 102.56350708007812, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 73, Perdida: 195.65447998046875, Exactitud: 0.5034375190734863, Perdida de prueba: 96.21346282958984, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 74, Perdida: 183.31808471679688, Exactitud: 0.5, Perdida de prueba: 82.38085174560547, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 75, Perdida: 160.59600830078125, Exactitud: 0.5159375071525574, Perdida de prueba: 83.20838928222656, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 76, Perdida: 152.35565185546875, Exactitud: 0.5096874833106995, Perdida de prueba: 77.95989990234375, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 77, Perdida: 149.65956115722656, Exactitud: 0.4793750047683716, Perdida de prueba: 79.54718017578125, Exactitud de prueba: 0.6997318863868713\n",
            "Epoch 78, Perdida: 141.84426879882812, Exactitud: 0.4881249964237213, Perdida de prueba: 64.58362579345703, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 79, Perdida: 125.36679077148438, Exactitud: 0.48906248807907104, Perdida de prueba: 66.08382415771484, Exactitud de prueba: 0.7024128437042236\n",
            "Epoch 80, Perdida: 118.54039001464844, Exactitud: 0.48093751072883606, Perdida de prueba: 59.473636627197266, Exactitud de prueba: 0.707774817943573\n",
            "Epoch 81, Perdida: 108.05908203125, Exactitud: 0.48374998569488525, Perdida de prueba: 55.209041595458984, Exactitud de prueba: 0.707774817943573\n",
            "Epoch 82, Perdida: 102.30532836914062, Exactitud: 0.4828124940395355, Perdida de prueba: 49.01958465576172, Exactitud de prueba: 0.7158176898956299\n",
            "Epoch 83, Perdida: 87.26951599121094, Exactitud: 0.4946874976158142, Perdida de prueba: 50.940956115722656, Exactitud de prueba: 0.6916890144348145\n",
            "Epoch 84, Perdida: 90.12774658203125, Exactitud: 0.4584375023841858, Perdida de prueba: 48.45138168334961, Exactitud de prueba: 0.6863270998001099\n",
            "Epoch 85, Perdida: 82.73971557617188, Exactitud: 0.46031248569488525, Perdida de prueba: 41.04438400268555, Exactitud de prueba: 0.707774817943573\n",
            "Epoch 86, Perdida: 74.60844421386719, Exactitud: 0.4737499952316284, Perdida de prueba: 36.66289138793945, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 87, Perdida: 65.89434051513672, Exactitud: 0.49281251430511475, Perdida de prueba: 32.032371520996094, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 88, Perdida: 56.58997344970703, Exactitud: 0.503125011920929, Perdida de prueba: 32.223777770996094, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 89, Perdida: 53.853675842285156, Exactitud: 0.4765625, Perdida de prueba: 34.294307708740234, Exactitud de prueba: 0.6836460828781128\n",
            "Epoch 90, Perdida: 54.79243850708008, Exactitud: 0.4593749940395355, Perdida de prueba: 29.178224563598633, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 91, Perdida: 49.24052429199219, Exactitud: 0.4740625023841858, Perdida de prueba: 26.336624145507812, Exactitud de prueba: 0.7050938606262207\n",
            "Epoch 92, Perdida: 44.63212585449219, Exactitud: 0.4712499976158142, Perdida de prueba: 22.743675231933594, Exactitud de prueba: 0.7158176898956299\n",
            "Epoch 93, Perdida: 38.926090240478516, Exactitud: 0.4753125011920929, Perdida de prueba: 23.6527099609375, Exactitud de prueba: 0.6943699717521667\n",
            "Epoch 94, Perdida: 38.73269271850586, Exactitud: 0.4621874988079071, Perdida de prueba: 20.279678344726562, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 95, Perdida: 33.823089599609375, Exactitud: 0.48124998807907104, Perdida de prueba: 16.855287551879883, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 96, Perdida: 28.02114486694336, Exactitud: 0.4918749928474426, Perdida de prueba: 17.475719451904297, Exactitud de prueba: 0.6890080571174622\n",
            "Epoch 97, Perdida: 27.853302001953125, Exactitud: 0.4625000059604645, Perdida de prueba: 18.258594512939453, Exactitud de prueba: 0.6756032109260559\n",
            "Epoch 98, Perdida: 28.226070404052734, Exactitud: 0.45656248927116394, Perdida de prueba: 13.824597358703613, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 99, Perdida: 22.35273551940918, Exactitud: 0.4828124940395355, Perdida de prueba: 12.551955223083496, Exactitud de prueba: 0.707774817943573\n",
            "Epoch 100, Perdida: 19.582935333251953, Exactitud: 0.4975000023841858, Perdida de prueba: 12.483292579650879, Exactitud de prueba: 0.6863270998001099\n",
            "Epoch 101, Perdida: 18.961854934692383, Exactitud: 0.47093749046325684, Perdida de prueba: 12.26609992980957, Exactitud de prueba: 0.6863270998001099\n",
            "Epoch 102, Perdida: 18.909765243530273, Exactitud: 0.46000000834465027, Perdida de prueba: 9.680113792419434, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 103, Perdida: 15.421049118041992, Exactitud: 0.48906248807907104, Perdida de prueba: 8.885171890258789, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 104, Perdida: 13.22484016418457, Exactitud: 0.48625001311302185, Perdida de prueba: 9.884305953979492, Exactitud de prueba: 0.667560338973999\n",
            "Epoch 105, Perdida: 14.578437805175781, Exactitud: 0.4493750035762787, Perdida de prueba: 7.7358784675598145, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 106, Perdida: 12.283721923828125, Exactitud: 0.5009375214576721, Perdida de prueba: 5.7744646072387695, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 107, Perdida: 8.48434066772461, Exactitud: 0.5278124809265137, Perdida de prueba: 7.957351207733154, Exactitud de prueba: 0.6380696892738342\n",
            "Epoch 108, Perdida: 10.314194679260254, Exactitud: 0.45625001192092896, Perdida de prueba: 7.590285778045654, Exactitud de prueba: 0.6916890144348145\n",
            "Epoch 109, Perdida: 11.35091781616211, Exactitud: 0.4740625023841858, Perdida de prueba: 4.403904914855957, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 110, Perdida: 5.6920576095581055, Exactitud: 0.5703125, Perdida de prueba: 5.190048694610596, Exactitud de prueba: 0.6380696892738342\n",
            "Epoch 111, Perdida: 6.450299263000488, Exactitud: 0.4962500035762787, Perdida de prueba: 7.581301212310791, Exactitud de prueba: 0.6407506465911865\n",
            "Epoch 112, Perdida: 10.185221672058105, Exactitud: 0.45500001311302185, Perdida de prueba: 4.042792320251465, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 113, Perdida: 3.974658489227295, Exactitud: 0.628125011920929, Perdida de prueba: 3.3840579986572266, Exactitud de prueba: 0.6407506465911865\n",
            "Epoch 114, Perdida: 4.284422397613525, Exactitud: 0.53125, Perdida de prueba: 7.511152744293213, Exactitud de prueba: 0.6166219711303711\n",
            "Epoch 115, Perdida: 8.99790096282959, Exactitud: 0.45124998688697815, Perdida de prueba: 4.066845893859863, Exactitud de prueba: 0.7024128437042236\n",
            "Epoch 116, Perdida: 2.5843379497528076, Exactitud: 0.7184374928474426, Perdida de prueba: 2.926612138748169, Exactitud de prueba: 0.6300268173217773\n",
            "Epoch 117, Perdida: 3.4243643283843994, Exactitud: 0.5365625023841858, Perdida de prueba: 6.13702392578125, Exactitud de prueba: 0.6219838857650757\n",
            "Epoch 118, Perdida: 7.2853546142578125, Exactitud: 0.4724999964237213, Perdida de prueba: 3.70149827003479, Exactitud de prueba: 0.707774817943573\n",
            "Epoch 119, Perdida: 1.6529463529586792, Exactitud: 0.7599999904632568, Perdida de prueba: 3.098813533782959, Exactitud de prueba: 0.6166219711303711\n",
            "Epoch 120, Perdida: 3.349546432495117, Exactitud: 0.5, Perdida de prueba: 3.994621753692627, Exactitud de prueba: 0.6782841682434082\n",
            "Epoch 121, Perdida: 4.928290367126465, Exactitud: 0.5337499976158142, Perdida de prueba: 2.485044479370117, Exactitud de prueba: 0.7104557752609253\n",
            "Epoch 122, Perdida: 1.1746344566345215, Exactitud: 0.7134374976158142, Perdida de prueba: 3.77795147895813, Exactitud de prueba: 0.5978552103042603\n",
            "Epoch 123, Perdida: 3.5469319820404053, Exactitud: 0.44062501192092896, Perdida de prueba: 2.6937906742095947, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 124, Perdida: 2.7223665714263916, Exactitud: 0.6575000286102295, Perdida de prueba: 1.4786977767944336, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 125, Perdida: 1.3070423603057861, Exactitud: 0.629687488079071, Perdida de prueba: 3.176508665084839, Exactitud de prueba: 0.6219838857650757\n",
            "Epoch 126, Perdida: 3.2547659873962402, Exactitud: 0.49312499165534973, Perdida de prueba: 1.9309340715408325, Exactitud de prueba: 0.7050938606262207\n",
            "Epoch 127, Perdida: 1.0839166641235352, Exactitud: 0.7168750166893005, Perdida de prueba: 2.1179733276367188, Exactitud de prueba: 0.6193029284477234\n",
            "Epoch 128, Perdida: 1.9771184921264648, Exactitud: 0.4675000011920929, Perdida de prueba: 1.8650363683700562, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 129, Perdida: 1.7905535697937012, Exactitud: 0.6225000023841858, Perdida de prueba: 1.130179762840271, Exactitud de prueba: 0.7158176898956299\n",
            "Epoch 130, Perdida: 1.0470585823059082, Exactitud: 0.6015625, Perdida de prueba: 1.918769359588623, Exactitud de prueba: 0.6595174074172974\n",
            "Epoch 131, Perdida: 1.96331787109375, Exactitud: 0.5284374952316284, Perdida de prueba: 1.2943212985992432, Exactitud de prueba: 0.7158176898956299\n",
            "Epoch 132, Perdida: 0.9929001927375793, Exactitud: 0.6340625286102295, Perdida de prueba: 1.42116379737854, Exactitud de prueba: 0.6461126208305359\n",
            "Epoch 133, Perdida: 1.4349355697631836, Exactitud: 0.5243750214576721, Perdida de prueba: 1.354745626449585, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 134, Perdida: 1.1619338989257812, Exactitud: 0.6459375023841858, Perdida de prueba: 1.1366792917251587, Exactitud de prueba: 0.6756032109260559\n",
            "Epoch 135, Perdida: 1.0832711458206177, Exactitud: 0.5662500262260437, Perdida de prueba: 1.3246902227401733, Exactitud de prueba: 0.7050938606262207\n",
            "Epoch 136, Perdida: 1.3006871938705444, Exactitud: 0.5962499976158142, Perdida de prueba: 1.0579677820205688, Exactitud de prueba: 0.7211796045303345\n",
            "Epoch 137, Perdida: 0.9521569609642029, Exactitud: 0.6187499761581421, Perdida de prueba: 1.185670018196106, Exactitud de prueba: 0.697050929069519\n",
            "Epoch 138, Perdida: 1.1191790103912354, Exactitud: 0.5924999713897705, Perdida de prueba: 1.0673320293426514, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 139, Perdida: 0.9010173678398132, Exactitud: 0.6253125071525574, Perdida de prueba: 1.060179591178894, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 140, Perdida: 0.9825562834739685, Exactitud: 0.6268749833106995, Perdida de prueba: 1.0365670919418335, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 141, Perdida: 0.8898534178733826, Exactitud: 0.6231250166893005, Perdida de prueba: 1.0444996356964111, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 142, Perdida: 0.9431746602058411, Exactitud: 0.6256250143051147, Perdida de prueba: 1.0037198066711426, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 143, Perdida: 0.8826567530632019, Exactitud: 0.6478124856948853, Perdida de prueba: 1.0088871717453003, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 144, Perdida: 0.8648971915245056, Exactitud: 0.653124988079071, Perdida de prueba: 0.9999930262565613, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 145, Perdida: 0.8270044922828674, Exactitud: 0.6518750190734863, Perdida de prueba: 0.9902741312980652, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 146, Perdida: 0.8182241916656494, Exactitud: 0.668749988079071, Perdida de prueba: 0.9687256813049316, Exactitud de prueba: 0.7131367325782776\n",
            "Epoch 147, Perdida: 0.7848054766654968, Exactitud: 0.6662499904632568, Perdida de prueba: 0.9625691771507263, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 148, Perdida: 0.7568000555038452, Exactitud: 0.6768749952316284, Perdida de prueba: 0.9596596956253052, Exactitud de prueba: 0.7211796045303345\n",
            "Epoch 149, Perdida: 0.7660537362098694, Exactitud: 0.6743749976158142, Perdida de prueba: 0.9621118903160095, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 150, Perdida: 0.7656466960906982, Exactitud: 0.6865624785423279, Perdida de prueba: 0.9387490153312683, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 151, Perdida: 0.7191082835197449, Exactitud: 0.6837499737739563, Perdida de prueba: 0.9485568404197693, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 152, Perdida: 0.73768550157547, Exactitud: 0.6953125, Perdida de prueba: 0.9423133730888367, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 153, Perdida: 0.7033510804176331, Exactitud: 0.6890624761581421, Perdida de prueba: 0.9480221271514893, Exactitud de prueba: 0.7211796045303345\n",
            "Epoch 154, Perdida: 0.7355844378471375, Exactitud: 0.684374988079071, Perdida de prueba: 0.9224289655685425, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 155, Perdida: 0.7044220566749573, Exactitud: 0.7006250023841858, Perdida de prueba: 0.9279187321662903, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 156, Perdida: 0.6770522594451904, Exactitud: 0.7015625238418579, Perdida de prueba: 0.9037894606590271, Exactitud de prueba: 0.7104557752609253\n",
            "Epoch 157, Perdida: 0.6812196373939514, Exactitud: 0.6987500190734863, Perdida de prueba: 0.925216555595398, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 158, Perdida: 0.6618315577507019, Exactitud: 0.7131249904632568, Perdida de prueba: 0.8972426652908325, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 159, Perdida: 0.6400906443595886, Exactitud: 0.7118750214576721, Perdida de prueba: 0.9199032783508301, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 160, Perdida: 0.6630926728248596, Exactitud: 0.7134374976158142, Perdida de prueba: 0.8957899212837219, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 161, Perdida: 0.6417368650436401, Exactitud: 0.71875, Perdida de prueba: 0.9052729606628418, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 162, Perdida: 0.6696142554283142, Exactitud: 0.7006250023841858, Perdida de prueba: 0.9153249859809875, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 163, Perdida: 0.6457177400588989, Exactitud: 0.7115625143051147, Perdida de prueba: 0.8831137418746948, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 164, Perdida: 0.6043062210083008, Exactitud: 0.7231249809265137, Perdida de prueba: 0.9075694680213928, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 165, Perdida: 0.6310301423072815, Exactitud: 0.7106249928474426, Perdida de prueba: 0.8834677934646606, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 166, Perdida: 0.6059951782226562, Exactitud: 0.721875011920929, Perdida de prueba: 0.902958869934082, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 167, Perdida: 0.6166470646858215, Exactitud: 0.7196875214576721, Perdida de prueba: 0.8747777938842773, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 168, Perdida: 0.5998754501342773, Exactitud: 0.731249988079071, Perdida de prueba: 0.9087556600570679, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 169, Perdida: 0.6239742636680603, Exactitud: 0.7231249809265137, Perdida de prueba: 0.8693671226501465, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 170, Perdida: 0.5940335988998413, Exactitud: 0.721875011920929, Perdida de prueba: 0.8821038603782654, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 171, Perdida: 0.6116881966590881, Exactitud: 0.7318750023841858, Perdida de prueba: 0.8761469721794128, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 172, Perdida: 0.5795219540596008, Exactitud: 0.731249988079071, Perdida de prueba: 0.8726736903190613, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 173, Perdida: 0.5755936503410339, Exactitud: 0.7340624928474426, Perdida de prueba: 0.8768640160560608, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 174, Perdida: 0.5769796371459961, Exactitud: 0.7378125190734863, Perdida de prueba: 0.8829525709152222, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 175, Perdida: 0.567956268787384, Exactitud: 0.7334374785423279, Perdida de prueba: 0.8714360594749451, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 176, Perdida: 0.5704896450042725, Exactitud: 0.739062488079071, Perdida de prueba: 0.8833884000778198, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 177, Perdida: 0.5471330285072327, Exactitud: 0.734375, Perdida de prueba: 0.8806025385856628, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 178, Perdida: 0.5602741241455078, Exactitud: 0.7340624928474426, Perdida de prueba: 0.8809247612953186, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 179, Perdida: 0.5431169867515564, Exactitud: 0.7421875, Perdida de prueba: 0.8764966726303101, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 180, Perdida: 0.5683732032775879, Exactitud: 0.7318750023841858, Perdida de prueba: 0.8721771836280823, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 181, Perdida: 0.5407204031944275, Exactitud: 0.7396875023841858, Perdida de prueba: 0.8641116619110107, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 182, Perdida: 0.54383385181427, Exactitud: 0.7393749952316284, Perdida de prueba: 0.8669115900993347, Exactitud de prueba: 0.7211796045303345\n",
            "Epoch 183, Perdida: 0.544434130191803, Exactitud: 0.7434375286102295, Perdida de prueba: 0.861430823802948, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 184, Perdida: 0.534581184387207, Exactitud: 0.7512500286102295, Perdida de prueba: 0.8783119916915894, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 185, Perdida: 0.5506929159164429, Exactitud: 0.7434375286102295, Perdida de prueba: 0.8534548878669739, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 186, Perdida: 0.5211460590362549, Exactitud: 0.7471874952316284, Perdida de prueba: 0.8581007719039917, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 187, Perdida: 0.5250734686851501, Exactitud: 0.7512500286102295, Perdida de prueba: 0.849758505821228, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 188, Perdida: 0.5327504873275757, Exactitud: 0.7381250262260437, Perdida de prueba: 0.8697144985198975, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 189, Perdida: 0.5137707591056824, Exactitud: 0.7509375214576721, Perdida de prueba: 0.8606349229812622, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 190, Perdida: 0.5003968477249146, Exactitud: 0.7571874856948853, Perdida de prueba: 0.8643336296081543, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 191, Perdida: 0.5206143260002136, Exactitud: 0.7412499785423279, Perdida de prueba: 0.8503151535987854, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 192, Perdida: 0.5200341939926147, Exactitud: 0.75, Perdida de prueba: 0.8562750816345215, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 193, Perdida: 0.5240049362182617, Exactitud: 0.7515624761581421, Perdida de prueba: 0.8617317080497742, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 194, Perdida: 0.5079286098480225, Exactitud: 0.7549999952316284, Perdida de prueba: 0.859602689743042, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 195, Perdida: 0.5295313000679016, Exactitud: 0.7381250262260437, Perdida de prueba: 0.8588643074035645, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 196, Perdida: 0.5002838373184204, Exactitud: 0.7603124976158142, Perdida de prueba: 0.8433680534362793, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 197, Perdida: 0.5115565061569214, Exactitud: 0.7475000023841858, Perdida de prueba: 0.8512130379676819, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 198, Perdida: 0.49830788373947144, Exactitud: 0.7543749809265137, Perdida de prueba: 0.8602440357208252, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 199, Perdida: 0.5086123943328857, Exactitud: 0.753125011920929, Perdida de prueba: 0.8489366769790649, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 200, Perdida: 0.4864776134490967, Exactitud: 0.7637500166893005, Perdida de prueba: 0.8929263949394226, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 201, Perdida: 0.5343379974365234, Exactitud: 0.7381250262260437, Perdida de prueba: 0.856975257396698, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 202, Perdida: 0.49821794033050537, Exactitud: 0.754687488079071, Perdida de prueba: 0.851206362247467, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 203, Perdida: 0.47016313672065735, Exactitud: 0.7659375071525574, Perdida de prueba: 0.860588014125824, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 204, Perdida: 0.4914224445819855, Exactitud: 0.7549999952316284, Perdida de prueba: 0.8720356822013855, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 205, Perdida: 0.4986451268196106, Exactitud: 0.7574999928474426, Perdida de prueba: 0.8497822284698486, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 206, Perdida: 0.4944075644016266, Exactitud: 0.7556250095367432, Perdida de prueba: 0.8594253063201904, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 207, Perdida: 0.49986863136291504, Exactitud: 0.7540624737739563, Perdida de prueba: 0.8519904613494873, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 208, Perdida: 0.5030314326286316, Exactitud: 0.7496874928474426, Perdida de prueba: 0.8555803298950195, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 209, Perdida: 0.4864129424095154, Exactitud: 0.7578125, Perdida de prueba: 0.8363795280456543, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 210, Perdida: 0.4976508617401123, Exactitud: 0.7606250047683716, Perdida de prueba: 0.8622508645057678, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 211, Perdida: 0.4992891848087311, Exactitud: 0.7506250143051147, Perdida de prueba: 0.827383816242218, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 212, Perdida: 0.4523355960845947, Exactitud: 0.7728124856948853, Perdida de prueba: 0.8629044890403748, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 213, Perdida: 0.48562702536582947, Exactitud: 0.7543749809265137, Perdida de prueba: 0.8552101850509644, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 214, Perdida: 0.4900311231613159, Exactitud: 0.7587500214576721, Perdida de prueba: 0.8472841382026672, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 215, Perdida: 0.48108503222465515, Exactitud: 0.7596874833106995, Perdida de prueba: 0.864979088306427, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 216, Perdida: 0.482035756111145, Exactitud: 0.7621874809265137, Perdida de prueba: 0.8684274554252625, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 217, Perdida: 0.4705114960670471, Exactitud: 0.7596874833106995, Perdida de prueba: 0.8574077486991882, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 218, Perdida: 0.4599752426147461, Exactitud: 0.7640625238418579, Perdida de prueba: 0.8685479164123535, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 219, Perdida: 0.48217475414276123, Exactitud: 0.7659375071525574, Perdida de prueba: 0.85711669921875, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 220, Perdida: 0.4470292031764984, Exactitud: 0.7674999833106995, Perdida de prueba: 0.8630750179290771, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 221, Perdida: 0.47134292125701904, Exactitud: 0.7599999904632568, Perdida de prueba: 0.867610514163971, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 222, Perdida: 0.46072542667388916, Exactitud: 0.7678124904632568, Perdida de prueba: 0.8725541234016418, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 223, Perdida: 0.4707651436328888, Exactitud: 0.7603124976158142, Perdida de prueba: 0.8594945073127747, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 224, Perdida: 0.4626065492630005, Exactitud: 0.762499988079071, Perdida de prueba: 0.8535648584365845, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 225, Perdida: 0.4573284685611725, Exactitud: 0.768750011920929, Perdida de prueba: 0.8591262698173523, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 226, Perdida: 0.45211946964263916, Exactitud: 0.7674999833106995, Perdida de prueba: 0.8712180852890015, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 227, Perdida: 0.4880291521549225, Exactitud: 0.7578125, Perdida de prueba: 0.8559436798095703, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 228, Perdida: 0.4423571825027466, Exactitud: 0.7762500047683716, Perdida de prueba: 0.8813032507896423, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 229, Perdida: 0.4493238627910614, Exactitud: 0.7731249928474426, Perdida de prueba: 0.8545393943786621, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 230, Perdida: 0.4424685537815094, Exactitud: 0.7721874713897705, Perdida de prueba: 0.8759180903434753, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 231, Perdida: 0.4700222611427307, Exactitud: 0.7637500166893005, Perdida de prueba: 0.8819422125816345, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 232, Perdida: 0.44587960839271545, Exactitud: 0.7678124904632568, Perdida de prueba: 0.866675078868866, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 233, Perdida: 0.4488050639629364, Exactitud: 0.7753124833106995, Perdida de prueba: 0.870270848274231, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 234, Perdida: 0.4350711405277252, Exactitud: 0.7793750166893005, Perdida de prueba: 0.8692302107810974, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 235, Perdida: 0.4535106420516968, Exactitud: 0.7706249952316284, Perdida de prueba: 0.8839378952980042, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 236, Perdida: 0.4398372769355774, Exactitud: 0.7806249856948853, Perdida de prueba: 0.875382661819458, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 237, Perdida: 0.45961883664131165, Exactitud: 0.7606250047683716, Perdida de prueba: 0.8867568969726562, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 238, Perdida: 0.4547719657421112, Exactitud: 0.770312488079071, Perdida de prueba: 0.8677545785903931, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 239, Perdida: 0.4226225018501282, Exactitud: 0.7840625047683716, Perdida de prueba: 0.8772574663162231, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 240, Perdida: 0.44840165972709656, Exactitud: 0.7756249904632568, Perdida de prueba: 0.9232216477394104, Exactitud de prueba: 0.7211796045303345\n",
            "Epoch 241, Perdida: 0.4837699830532074, Exactitud: 0.7587500214576721, Perdida de prueba: 0.8714823722839355, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 242, Perdida: 0.43349117040634155, Exactitud: 0.7774999737739563, Perdida de prueba: 0.8779022097587585, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 243, Perdida: 0.4440317153930664, Exactitud: 0.7718750238418579, Perdida de prueba: 0.8683300614356995, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 244, Perdida: 0.4435228109359741, Exactitud: 0.776562511920929, Perdida de prueba: 0.8948879837989807, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 245, Perdida: 0.4578271210193634, Exactitud: 0.7653124928474426, Perdida de prueba: 0.8775266408920288, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 246, Perdida: 0.4491257965564728, Exactitud: 0.7709375023841858, Perdida de prueba: 0.8970119953155518, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 247, Perdida: 0.441615492105484, Exactitud: 0.7709375023841858, Perdida de prueba: 0.880703866481781, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 248, Perdida: 0.44598206877708435, Exactitud: 0.7746875286102295, Perdida de prueba: 0.8841846585273743, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 249, Perdida: 0.4510343372821808, Exactitud: 0.7696874737739563, Perdida de prueba: 0.8922898173332214, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 250, Perdida: 0.4520075023174286, Exactitud: 0.7699999809265137, Perdida de prueba: 0.8983680605888367, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 251, Perdida: 0.44353240728378296, Exactitud: 0.7806249856948853, Perdida de prueba: 0.904608428478241, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 252, Perdida: 0.4334733784198761, Exactitud: 0.7784374952316284, Perdida de prueba: 0.8945404291152954, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 253, Perdida: 0.44058966636657715, Exactitud: 0.7790625095367432, Perdida de prueba: 0.8764367699623108, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 254, Perdida: 0.4441434144973755, Exactitud: 0.7690625190734863, Perdida de prueba: 0.8931852579116821, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 255, Perdida: 0.4658154249191284, Exactitud: 0.7674999833106995, Perdida de prueba: 0.8899892568588257, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 256, Perdida: 0.459903359413147, Exactitud: 0.765625, Perdida de prueba: 0.8880850076675415, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 257, Perdida: 0.45242854952812195, Exactitud: 0.770312488079071, Perdida de prueba: 0.8778011798858643, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 258, Perdida: 0.4609360098838806, Exactitud: 0.7646874785423279, Perdida de prueba: 0.8887954950332642, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 259, Perdida: 0.44050049781799316, Exactitud: 0.7746875286102295, Perdida de prueba: 0.9086147546768188, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 260, Perdida: 0.435428649187088, Exactitud: 0.7774999737739563, Perdida de prueba: 0.8965882658958435, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 261, Perdida: 0.4433545172214508, Exactitud: 0.7746875286102295, Perdida de prueba: 0.9041486978530884, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 262, Perdida: 0.4273337721824646, Exactitud: 0.7796875238418579, Perdida de prueba: 0.9115978479385376, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 263, Perdida: 0.45041871070861816, Exactitud: 0.7631250023841858, Perdida de prueba: 0.8902460932731628, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 264, Perdida: 0.45529499650001526, Exactitud: 0.7643749713897705, Perdida de prueba: 0.8844519257545471, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 265, Perdida: 0.46093177795410156, Exactitud: 0.7718750238418579, Perdida de prueba: 0.8792614936828613, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 266, Perdida: 0.43846553564071655, Exactitud: 0.7743750214576721, Perdida de prueba: 0.8862634301185608, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 267, Perdida: 0.45530134439468384, Exactitud: 0.7674999833106995, Perdida de prueba: 0.8890551924705505, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 268, Perdida: 0.4479641020298004, Exactitud: 0.7706249952316284, Perdida de prueba: 0.8781620264053345, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 269, Perdida: 0.4446173906326294, Exactitud: 0.765625, Perdida de prueba: 0.8942641019821167, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 270, Perdida: 0.47989219427108765, Exactitud: 0.7596874833106995, Perdida de prueba: 0.9108964204788208, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 271, Perdida: 0.4643794000148773, Exactitud: 0.7674999833106995, Perdida de prueba: 0.882849395275116, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 272, Perdida: 0.44258880615234375, Exactitud: 0.7762500047683716, Perdida de prueba: 0.8773999810218811, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 273, Perdida: 0.4515341818332672, Exactitud: 0.7634375095367432, Perdida de prueba: 0.88184654712677, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 274, Perdida: 0.4610244333744049, Exactitud: 0.7606250047683716, Perdida de prueba: 0.8747671246528625, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 275, Perdida: 0.4482443928718567, Exactitud: 0.7728124856948853, Perdida de prueba: 0.8878077864646912, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 276, Perdida: 0.4259908199310303, Exactitud: 0.7756249904632568, Perdida de prueba: 0.9046595096588135, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 277, Perdida: 0.45758217573165894, Exactitud: 0.7615625262260437, Perdida de prueba: 0.891740083694458, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 278, Perdida: 0.43336546421051025, Exactitud: 0.778124988079071, Perdida de prueba: 0.9022851586341858, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 279, Perdida: 0.43410924077033997, Exactitud: 0.7696874737739563, Perdida de prueba: 0.9152836203575134, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 280, Perdida: 0.4479890763759613, Exactitud: 0.7728124856948853, Perdida de prueba: 0.8953499794006348, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 281, Perdida: 0.42481857538223267, Exactitud: 0.7799999713897705, Perdida de prueba: 0.9145033359527588, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 282, Perdida: 0.436533123254776, Exactitud: 0.785937488079071, Perdida de prueba: 0.9396349787712097, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 283, Perdida: 0.44069644808769226, Exactitud: 0.7731249928474426, Perdida de prueba: 0.9255926609039307, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 284, Perdida: 0.42996734380722046, Exactitud: 0.7768750190734863, Perdida de prueba: 0.9096407890319824, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 285, Perdida: 0.41115379333496094, Exactitud: 0.7893750071525574, Perdida de prueba: 0.932844877243042, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 286, Perdida: 0.44790735840797424, Exactitud: 0.7690625190734863, Perdida de prueba: 0.9164872765541077, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 287, Perdida: 0.4356662333011627, Exactitud: 0.7721874713897705, Perdida de prueba: 0.9146175384521484, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 288, Perdida: 0.43248090147972107, Exactitud: 0.7778124809265137, Perdida de prueba: 0.9050455093383789, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 289, Perdida: 0.43060705065727234, Exactitud: 0.7784374952316284, Perdida de prueba: 0.9421149492263794, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 290, Perdida: 0.4659241735935211, Exactitud: 0.7640625238418579, Perdida de prueba: 0.9003782868385315, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 291, Perdida: 0.4243728220462799, Exactitud: 0.7815625071525574, Perdida de prueba: 0.9049087762832642, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 292, Perdida: 0.4386133551597595, Exactitud: 0.7737500071525574, Perdida de prueba: 0.910578727722168, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 293, Perdida: 0.44242578744888306, Exactitud: 0.7740625143051147, Perdida de prueba: 0.9069593548774719, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 294, Perdida: 0.4437834322452545, Exactitud: 0.7718750238418579, Perdida de prueba: 0.8976725339889526, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 295, Perdida: 0.429220974445343, Exactitud: 0.7768750190734863, Perdida de prueba: 0.8961933255195618, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 296, Perdida: 0.43123719096183777, Exactitud: 0.778124988079071, Perdida de prueba: 0.9102133512496948, Exactitud de prueba: 0.7587131261825562\n",
            "Epoch 297, Perdida: 0.4326442778110504, Exactitud: 0.78125, Perdida de prueba: 0.9040682911872864, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 298, Perdida: 0.43315520882606506, Exactitud: 0.7728124856948853, Perdida de prueba: 0.9302932620048523, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 299, Perdida: 0.439508318901062, Exactitud: 0.7693750262260437, Perdida de prueba: 0.9018319845199585, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 300, Perdida: 0.42829400300979614, Exactitud: 0.7787500023841858, Perdida de prueba: 0.9016487002372742, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 301, Perdida: 0.4387243986129761, Exactitud: 0.7749999761581421, Perdida de prueba: 0.9280257225036621, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 302, Perdida: 0.44168221950531006, Exactitud: 0.7793750166893005, Perdida de prueba: 0.8943279385566711, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 303, Perdida: 0.420463502407074, Exactitud: 0.7778124809265137, Perdida de prueba: 1.0542194843292236, Exactitud de prueba: 0.7158176898956299\n",
            "Epoch 304, Perdida: 0.4887371361255646, Exactitud: 0.7674999833106995, Perdida de prueba: 0.9432136416435242, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 305, Perdida: 0.43430188298225403, Exactitud: 0.7737500071525574, Perdida de prueba: 0.9123859405517578, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 306, Perdida: 0.43188580870628357, Exactitud: 0.7740625143051147, Perdida de prueba: 0.9229569435119629, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 307, Perdida: 0.4365897476673126, Exactitud: 0.7734375, Perdida de prueba: 0.8951324224472046, Exactitud de prueba: 0.7587131261825562\n",
            "Epoch 308, Perdida: 0.4222917854785919, Exactitud: 0.7871875166893005, Perdida de prueba: 0.952512264251709, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 309, Perdida: 0.4730728566646576, Exactitud: 0.7593749761581421, Perdida de prueba: 0.8940972685813904, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 310, Perdida: 0.41758227348327637, Exactitud: 0.7828124761581421, Perdida de prueba: 0.9370325207710266, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 311, Perdida: 0.4474013149738312, Exactitud: 0.7737500071525574, Perdida de prueba: 0.8879610300064087, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 312, Perdida: 0.4272266924381256, Exactitud: 0.778124988079071, Perdida de prueba: 0.9129387736320496, Exactitud de prueba: 0.7640750408172607\n",
            "Epoch 313, Perdida: 0.4326190948486328, Exactitud: 0.7809374928474426, Perdida de prueba: 0.9024069309234619, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 314, Perdida: 0.443691223859787, Exactitud: 0.7618749737739563, Perdida de prueba: 0.914071261882782, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 315, Perdida: 0.4376031458377838, Exactitud: 0.7734375, Perdida de prueba: 0.9149413704872131, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 316, Perdida: 0.4307624399662018, Exactitud: 0.7715625166893005, Perdida de prueba: 0.9155163764953613, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 317, Perdida: 0.42086923122406006, Exactitud: 0.7881249785423279, Perdida de prueba: 0.9339740872383118, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 318, Perdida: 0.42944151163101196, Exactitud: 0.7721874713897705, Perdida de prueba: 0.9055957794189453, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 319, Perdida: 0.41418513655662537, Exactitud: 0.7803124785423279, Perdida de prueba: 0.9787838459014893, Exactitud de prueba: 0.7131367325782776\n",
            "Epoch 320, Perdida: 0.45429784059524536, Exactitud: 0.7674999833106995, Perdida de prueba: 0.9600944519042969, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 321, Perdida: 0.4340675473213196, Exactitud: 0.7737500071525574, Perdida de prueba: 0.9162880778312683, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 322, Perdida: 0.42097166180610657, Exactitud: 0.7837499976158142, Perdida de prueba: 0.9308946132659912, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 323, Perdida: 0.4282696545124054, Exactitud: 0.7734375, Perdida de prueba: 0.9512620568275452, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 324, Perdida: 0.4551725387573242, Exactitud: 0.7665625214576721, Perdida de prueba: 0.927912175655365, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 325, Perdida: 0.4069080948829651, Exactitud: 0.7868750095367432, Perdida de prueba: 0.9256157875061035, Exactitud de prueba: 0.7613940834999084\n",
            "Epoch 326, Perdida: 0.43229275941848755, Exactitud: 0.7721874713897705, Perdida de prueba: 0.9231194257736206, Exactitud de prueba: 0.7747989296913147\n",
            "Epoch 327, Perdida: 0.43497827649116516, Exactitud: 0.7749999761581421, Perdida de prueba: 0.951120138168335, Exactitud de prueba: 0.7587131261825562\n",
            "Epoch 328, Perdida: 0.4316178560256958, Exactitud: 0.7799999713897705, Perdida de prueba: 0.9521452188491821, Exactitud de prueba: 0.7640750408172607\n",
            "Epoch 329, Perdida: 0.4293804466724396, Exactitud: 0.7806249856948853, Perdida de prueba: 0.9361552000045776, Exactitud de prueba: 0.7613940834999084\n",
            "Epoch 330, Perdida: 0.42439132928848267, Exactitud: 0.7778124809265137, Perdida de prueba: 0.9353442788124084, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 331, Perdida: 0.41104546189308167, Exactitud: 0.7850000262260437, Perdida de prueba: 0.9585996866226196, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 332, Perdida: 0.4325263202190399, Exactitud: 0.7806249856948853, Perdida de prueba: 0.9670541286468506, Exactitud de prueba: 0.7158176898956299\n",
            "Epoch 333, Perdida: 0.44312405586242676, Exactitud: 0.776562511920929, Perdida de prueba: 0.978201150894165, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 334, Perdida: 0.40849584341049194, Exactitud: 0.7871875166893005, Perdida de prueba: 0.9335569143295288, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 335, Perdida: 0.414326548576355, Exactitud: 0.7865625023841858, Perdida de prueba: 0.9660502076148987, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 336, Perdida: 0.4060446321964264, Exactitud: 0.7940624952316284, Perdida de prueba: 0.9531965255737305, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 337, Perdida: 0.41484400629997253, Exactitud: 0.7884374856948853, Perdida de prueba: 0.9638834595680237, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 338, Perdida: 0.42491355538368225, Exactitud: 0.7756249904632568, Perdida de prueba: 0.972896933555603, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 339, Perdida: 0.4208891987800598, Exactitud: 0.7881249785423279, Perdida de prueba: 0.9778097867965698, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 340, Perdida: 0.43357518315315247, Exactitud: 0.7778124809265137, Perdida de prueba: 0.9735161662101746, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 341, Perdida: 0.42159825563430786, Exactitud: 0.7796875238418579, Perdida de prueba: 0.9438799023628235, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 342, Perdida: 0.42105427384376526, Exactitud: 0.7878124713897705, Perdida de prueba: 0.9646329879760742, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 343, Perdida: 0.423393189907074, Exactitud: 0.7878124713897705, Perdida de prueba: 0.9566695094108582, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 344, Perdida: 0.40969595313072205, Exactitud: 0.7918750047683716, Perdida de prueba: 0.9591973423957825, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 345, Perdida: 0.4120508134365082, Exactitud: 0.7862499952316284, Perdida de prueba: 0.9619745016098022, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 346, Perdida: 0.41453200578689575, Exactitud: 0.7831249833106995, Perdida de prueba: 0.95830899477005, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 347, Perdida: 0.40620267391204834, Exactitud: 0.7918750047683716, Perdida de prueba: 1.0129081010818481, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 348, Perdida: 0.4293101727962494, Exactitud: 0.7768750190734863, Perdida de prueba: 0.9690036177635193, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 349, Perdida: 0.39095252752304077, Exactitud: 0.7918750047683716, Perdida de prueba: 0.9563128352165222, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 350, Perdida: 0.4145110845565796, Exactitud: 0.7878124713897705, Perdida de prueba: 0.9817104935646057, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 351, Perdida: 0.4153892695903778, Exactitud: 0.7890625, Perdida de prueba: 0.9720302820205688, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 352, Perdida: 0.4121759831905365, Exactitud: 0.7875000238418579, Perdida de prueba: 0.9824210405349731, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 353, Perdida: 0.42216306924819946, Exactitud: 0.7850000262260437, Perdida de prueba: 0.9890170097351074, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 354, Perdida: 0.4110751450061798, Exactitud: 0.7900000214576721, Perdida de prueba: 0.9857383966445923, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 355, Perdida: 0.41484934091567993, Exactitud: 0.7868750095367432, Perdida de prueba: 0.9820659160614014, Exactitud de prueba: 0.7238605618476868\n",
            "Epoch 356, Perdida: 0.4078666567802429, Exactitud: 0.7925000190734863, Perdida de prueba: 0.9869576096534729, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 357, Perdida: 0.3987005054950714, Exactitud: 0.7984374761581421, Perdida de prueba: 1.0014662742614746, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 358, Perdida: 0.41538020968437195, Exactitud: 0.785937488079071, Perdida de prueba: 0.9905821681022644, Exactitud de prueba: 0.7613940834999084\n",
            "Epoch 359, Perdida: 0.39196252822875977, Exactitud: 0.8009374737739563, Perdida de prueba: 0.9917795062065125, Exactitud de prueba: 0.7613940834999084\n",
            "Epoch 360, Perdida: 0.4053176939487457, Exactitud: 0.7943750023841858, Perdida de prueba: 1.0150322914123535, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 361, Perdida: 0.40949657559394836, Exactitud: 0.7906249761581421, Perdida de prueba: 0.9971460103988647, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 362, Perdida: 0.40889185667037964, Exactitud: 0.7909374833106995, Perdida de prueba: 0.9967390298843384, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 363, Perdida: 0.4066223204135895, Exactitud: 0.7906249761581421, Perdida de prueba: 0.9899399280548096, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 364, Perdida: 0.41302236914634705, Exactitud: 0.785937488079071, Perdida de prueba: 1.0078182220458984, Exactitud de prueba: 0.7506702542304993\n",
            "Epoch 365, Perdida: 0.3969435691833496, Exactitud: 0.8006250262260437, Perdida de prueba: 1.0101439952850342, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 366, Perdida: 0.39341050386428833, Exactitud: 0.7943750023841858, Perdida de prueba: 1.016985535621643, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 367, Perdida: 0.4412887990474701, Exactitud: 0.7881249785423279, Perdida de prueba: 0.9921218156814575, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 368, Perdida: 0.3990643322467804, Exactitud: 0.7893750071525574, Perdida de prueba: 1.025653600692749, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 369, Perdida: 0.40752193331718445, Exactitud: 0.7925000190734863, Perdida de prueba: 0.9997577667236328, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 370, Perdida: 0.3953154683113098, Exactitud: 0.7931249737739563, Perdida de prueba: 1.0122829675674438, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 371, Perdida: 0.3832036554813385, Exactitud: 0.8021875023841858, Perdida de prueba: 0.9997442364692688, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 372, Perdida: 0.40244337916374207, Exactitud: 0.7959374785423279, Perdida de prueba: 1.0102142095565796, Exactitud de prueba: 0.7345844507217407\n",
            "Epoch 373, Perdida: 0.397602379322052, Exactitud: 0.7943750023841858, Perdida de prueba: 1.0139288902282715, Exactitud de prueba: 0.7587131261825562\n",
            "Epoch 374, Perdida: 0.39694908261299133, Exactitud: 0.8012499809265137, Perdida de prueba: 1.0372469425201416, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 375, Perdida: 0.4039364457130432, Exactitud: 0.7959374785423279, Perdida de prueba: 1.0089757442474365, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 376, Perdida: 0.38732045888900757, Exactitud: 0.7965624928474426, Perdida de prueba: 1.0079493522644043, Exactitud de prueba: 0.7640750408172607\n",
            "Epoch 377, Perdida: 0.3793146312236786, Exactitud: 0.8056250214576721, Perdida de prueba: 1.0519546270370483, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 378, Perdida: 0.39934229850769043, Exactitud: 0.796875, Perdida de prueba: 1.0215808153152466, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 379, Perdida: 0.3950037360191345, Exactitud: 0.7934374809265137, Perdida de prueba: 1.097209095954895, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 380, Perdida: 0.4300737679004669, Exactitud: 0.7906249761581421, Perdida de prueba: 0.9858765006065369, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 381, Perdida: 0.39085960388183594, Exactitud: 0.7987499833106995, Perdida de prueba: 1.0282695293426514, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 382, Perdida: 0.4189930558204651, Exactitud: 0.7884374856948853, Perdida de prueba: 1.0312182903289795, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 383, Perdida: 0.4305919110774994, Exactitud: 0.7821875214576721, Perdida de prueba: 0.9909254908561707, Exactitud de prueba: 0.7640750408172607\n",
            "Epoch 384, Perdida: 0.4064309597015381, Exactitud: 0.7862499952316284, Perdida de prueba: 1.0283007621765137, Exactitud de prueba: 0.7426273226737976\n",
            "Epoch 385, Perdida: 0.42053723335266113, Exactitud: 0.7946875095367432, Perdida de prueba: 1.0362977981567383, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 386, Perdida: 0.4246658980846405, Exactitud: 0.7821875214576721, Perdida de prueba: 1.0358999967575073, Exactitud de prueba: 0.7319034934043884\n",
            "Epoch 387, Perdida: 0.42079901695251465, Exactitud: 0.7928125262260437, Perdida de prueba: 1.0660206079483032, Exactitud de prueba: 0.7184986472129822\n",
            "Epoch 388, Perdida: 0.41888919472694397, Exactitud: 0.7887499928474426, Perdida de prueba: 1.0912907123565674, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 389, Perdida: 0.4453136920928955, Exactitud: 0.7731249928474426, Perdida de prueba: 1.023058295249939, Exactitud de prueba: 0.7560321688652039\n",
            "Epoch 390, Perdida: 0.42298007011413574, Exactitud: 0.7796875238418579, Perdida de prueba: 1.0389010906219482, Exactitud de prueba: 0.7613940834999084\n",
            "Epoch 391, Perdida: 0.42045077681541443, Exactitud: 0.7862499952316284, Perdida de prueba: 1.062874674797058, Exactitud de prueba: 0.7292225360870361\n",
            "Epoch 392, Perdida: 0.4146505296230316, Exactitud: 0.7934374809265137, Perdida de prueba: 1.0345189571380615, Exactitud de prueba: 0.747989296913147\n",
            "Epoch 393, Perdida: 0.414164662361145, Exactitud: 0.785937488079071, Perdida de prueba: 1.0385420322418213, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 394, Perdida: 0.4111175537109375, Exactitud: 0.7896875143051147, Perdida de prueba: 1.0355720520019531, Exactitud de prueba: 0.7453083395957947\n",
            "Epoch 395, Perdida: 0.40651288628578186, Exactitud: 0.8040624856948853, Perdida de prueba: 1.040105938911438, Exactitud de prueba: 0.7265415787696838\n",
            "Epoch 396, Perdida: 0.4237896502017975, Exactitud: 0.7837499976158142, Perdida de prueba: 1.0041593313217163, Exactitud de prueba: 0.7533512115478516\n",
            "Epoch 397, Perdida: 0.40241825580596924, Exactitud: 0.7971875071525574, Perdida de prueba: 1.0450289249420166, Exactitud de prueba: 0.7587131261825562\n",
            "Epoch 398, Perdida: 0.4462966322898865, Exactitud: 0.7778124809265137, Perdida de prueba: 1.1006321907043457, Exactitud de prueba: 0.7399463653564453\n",
            "Epoch 399, Perdida: 0.4337702691555023, Exactitud: 0.7871875166893005, Perdida de prueba: 1.047938585281372, Exactitud de prueba: 0.737265408039093\n",
            "Epoch 400, Perdida: 0.39918673038482666, Exactitud: 0.7987499833106995, Perdida de prueba: 1.0390640497207642, Exactitud de prueba: 0.7292225360870361\n"
          ]
        }
      ],
      "source": [
        "fitting(DNN,x_train,y_train_onehot,x_test,y_test_onehot,400,100,200, .7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Declaramos la funcion de entrenamiento\n",
        "def fitting2(model,train_x,train_y,test_x,test_y,EPOCHS,N_batch,batch_size, rate):\n",
        "  \"\"\"\n",
        "  Esta función lleva a cabo el entrenamiento de una red neuronal\n",
        "  Recibe:\n",
        "  * model.- La red neuronal a entrenar. Debe ser un objeto de la clase \"DNN_model\"\n",
        "  * train_x.- El conjunto de datos de entrenamiento.\n",
        "  * train_y.- El conjutno de etiquetas de entrenamiento.\n",
        "  * test_x.- El conjunto de datos de prueba.\n",
        "  * test_y.- El conjunto de etiquetas de prueba.\n",
        "  * EPOCHS.- El número de épocas de entrenamiento.\n",
        "  * N_batch.- El número de lotes (batches) a procesar\n",
        "  * bacth_size.- El tamaño de cada lote (batch)\n",
        "\n",
        "  No entrega nada como salida pero muestra en pantalla el proceso del entrenamiento\n",
        "  \"\"\"\n",
        "  #El ciclo de numero de epocas\n",
        "  img = np.reshape(train_x[0], (-1, 1, 512, 1))\n",
        "  with train_summary_writer.as_default():\n",
        "\n",
        "    tf.summary.image(\"Training data\", img, step=0)\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    i=0\n",
        "    #El ciclo por los batches...\n",
        "    while i+batch_size < len(train_x): #or i+batch_size<batch_size*N_batch:\n",
        "      #Calculamos el inicio y fin del lote...\n",
        "      start = i\n",
        "      end = i+batch_size\n",
        "      #Tomamos la rebanada en train_x y train_y a propagar por la red...\n",
        "      batch_x = train_x[start:end]\n",
        "      batch_y = train_y[start:end]\n",
        "      #Hacemos el paso de entrenamiento, enviamos la red, los datos y sus etiquetas...\n",
        "      train_step(model,batch_x,batch_y, rate)\n",
        "      #Hacemos el incremento en el contador del lote...\n",
        "      i+=batch_size\n",
        "    #Hacemos el paso de prueba...\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
        "      tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
        "\n",
        "\n",
        "\n",
        "    tf.summary.trace_on(graph=True, profiler=True)\n",
        "    test_step(model,test_x,test_y)\n",
        "    #print('x', test_x[1], 'y', test_y[1])\n",
        "\n",
        "\n",
        "    with train_summary_writer.as_default():\n",
        "      tf.summary.trace_export(\n",
        "        name=\"function_trace\", step=0, profiler_outdir= train_log_dir)\n",
        "\n",
        "    with test_summary_writer.as_default():\n",
        "      tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
        "      tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
        "\n",
        "    #Desplegamos en pantalla la epoca, la pérdida y el error de entrenamient y de prueba...\n",
        "    template = '{}'\n",
        "    print(template.format(\n",
        "                        test_accuracy.result()))\n",
        "\n",
        "    #Reseteamos las metricas para que no se acumule la información de todas las épocas...\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_loss.reset_states()\n",
        "    test_accuracy.reset_states()\n",
        "    test_fn.reset_state()\n",
        ""
      ],
      "metadata": {
        "id": "UzeoWfkAkJm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "eK8H4qVW0qDX",
        "outputId": "9ae35b83-89a1-4dba-82f5-cd51e3fcb358"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-4e17ed7c0274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;31m#sentence_test= sentence_test_total[K]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mfitting2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-230fc1e4c6b3>\u001b[0m in \u001b[0;36mfitting2\u001b[0;34m(model, train_x, train_y, test_x, test_y, EPOCHS, N_batch, batch_size, rate)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \"\"\"\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m#El ciclo de numero de epocas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    296\u001b[0m            [5, 6]])\n\u001b[1;32m    297\u001b[0m     \"\"\"\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 768 into shape (1,512,1)"
          ]
        }
      ],
      "source": [
        "#K=9\n",
        "#x_train= x_train_total[K]\n",
        "#y_train_onehot=y_train_onehot_total[K]\n",
        "#sentence_train=sentence_train_total[K]\n",
        "#x_test= x_test_total[K]\n",
        "#y_test_onehot= y_test_onehot_total[K]\n",
        "#sentence_test= sentence_test_total[K]\n",
        "\n",
        "for i in range (10):\n",
        "  K=i\n",
        "  x_train= x_train_total[K]\n",
        "  y_train_onehot=y_train_onehot_total[K]\n",
        "  #sentence_train=sentence_train_total[K]\n",
        "  x_test= x_test_total[K]\n",
        "  y_test_onehot= y_test_onehot_total[K]\n",
        "  #sentence_test= sentence_test_total[K]\n",
        "\n",
        "  fitting2(DNN,x_train,y_train_onehot,x_test,y_test_onehot,1,100,200, .7)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def metricas(x, y, rate, k):\n",
        "\n",
        "  ##se declaran las métricas\n",
        "  test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
        "  test_fn= tf.keras.metrics.FalseNegatives(name= 'test_fn')\n",
        "  test_fp=tf.keras.metrics.FalsePositives(name= 'test_fp')\n",
        "  test_tn= tf.keras.metrics.TrueNegatives(name= 'test_tn')\n",
        "  test_tp=tf.keras.metrics.TruePositives(name='test_tp')\n",
        "  test_precision =tf.keras.metrics.Precision(name='test_precision')\n",
        "  test_recall=tf.keras.metrics.Recall(name='test_recall')\n",
        "\n",
        "\n",
        "  pred= DNN(x, rate)\n",
        "  new=[]\n",
        "  lit=0\n",
        "  met=0\n",
        "\n",
        "  ##0 es literal y 1 es metafora\n",
        "  for i in range(len(pred)):\n",
        "    if pred[i][0]>=pred[i][1]:\n",
        "      new.append(0)\n",
        "      lit= lit+1\n",
        "    if pred[i][0]<pred[i][1]:\n",
        "      new.append(1)\n",
        "      met= met+1\n",
        "\n",
        "  labels= []\n",
        "  for j in range (len(y)):\n",
        "    if y_test_onehot[j][0]==1.:\n",
        "      labels.append(0)\n",
        "      met= met+1\n",
        "    else:\n",
        "      labels.append(1)\n",
        "      lit= lit+1\n",
        "\n",
        "  print(labels[0:10])\n",
        "  print(new[0:10])\n",
        "\n",
        "  print(len(x_test), met, lit)\n",
        "\n",
        "\n",
        "  test_accuracy(y_test_onehot, pred)\n",
        "  test_fn.update_state(labels, new)\n",
        "  test_fp.update_state(labels, new)\n",
        "  test_tp.update_state(labels, new)\n",
        "  test_tn.update_state(labels, new)\n",
        "  test_precision.update_state(labels, new)\n",
        "  test_recall.update_state(labels, new)\n",
        "\n",
        "  print('Para k:', k)\n",
        "  print('Acuracy', test_accuracy.result())\n",
        "  print('False negative', test_fn.result())\n",
        "  print('False positive', test_fp.result())\n",
        "  print('True Negative', test_tn.result())\n",
        "  print('True positive', test_tp.result())\n",
        "  print('Presición', test_precision.result())\n",
        "  print('recall', test_recall.result())\n",
        "\n",
        "  #print(labels[1], new[1])\n",
        "\n",
        "\n",
        "  test_accuracy.reset_states()\n",
        "  test_fp.reset_states()\n",
        "  test_fn.reset_states()\n",
        "  test_tp.reset_states()\n",
        "  test_tn.reset_states()\n",
        "  test_precision.reset_states()\n",
        "  test_recall.reset_states()"
      ],
      "metadata": {
        "id": "suex-F-Zb2JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (10):\n",
        "  K=i\n",
        "  x_train= x_train_total[K]\n",
        "  y_train_onehot=y_train_onehot_total[K]\n",
        "  #sentence_train=sentence_train_total[K]\n",
        "  x_test= x_test_total[K]\n",
        "  y_test_onehot= y_test_onehot_total[K]\n",
        "  #sentence_test= sentence_test_total[K]\n",
        "\n",
        "  metricas(x_test,y_test_onehot, .7, K)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Qu2axdcSts1",
        "outputId": "8f2f77b6-e22d-4c7f-e330-5bfbe09f15b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
            "373 407 339\n",
            "Para k: 0\n",
            "Acuracy tf.Tensor(0.7211796, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(35.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(69.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(91.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(178.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.72064775, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.8356807, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 0, 0, 1, 0, 0, 0, 1]\n",
            "373 424 322\n",
            "Para k: 1\n",
            "Acuracy tf.Tensor(0.74530834, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(22.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(73.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(87.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(191.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.7234849, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.8967136, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
            "373 416 330\n",
            "Para k: 2\n",
            "Acuracy tf.Tensor(0.73994637, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(27.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(70.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(90.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(186.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.7265625, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.87323946, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
            "373 412 334\n",
            "Para k: 3\n",
            "Acuracy tf.Tensor(0.73994637, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(29.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(68.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(92.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(184.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.73015875, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.86384976, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "373 426 320\n",
            "Para k: 4\n",
            "Acuracy tf.Tensor(0.71849865, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(26.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(79.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(81.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(187.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.7030075, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.8779343, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 0, 0, 1, 0, 0, 0, 0]\n",
            "373 413 333\n",
            "Para k: 5\n",
            "Acuracy tf.Tensor(0.7319035, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(30.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(70.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(90.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(183.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.7233202, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.85915494, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 0, 0, 0, 0, 0, 1]\n",
            "373 418 328\n",
            "Para k: 6\n",
            "Acuracy tf.Tensor(0.71849865, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(30.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(75.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(85.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(183.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.7093023, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.85915494, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
            "373 419 327\n",
            "Para k: 7\n",
            "Acuracy tf.Tensor(0.7319035, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(27.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(73.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(87.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(186.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.71814674, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.87323946, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 0, 0, 0, 0, 0, 0, 1]\n",
            "373 411 335\n",
            "Para k: 8\n",
            "Acuracy tf.Tensor(0.7211796, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(33.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(71.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(89.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(180.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.7171315, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.8450704, shape=(), dtype=float32)\n",
            "x (373, 1, 768)\n",
            "img: (373, 1, 768, 1)\n",
            "l1 (373, 1, 384, 32)\n",
            "l2 (373, 1, 192, 64)\n",
            "l3 (373, 1, 96, 128)\n",
            "(373, 1, 48, 256) l4\n",
            "(373, 1, 24, 512) l5\n",
            "(373, 1, 12, 512) l6\n",
            "input shape: (373, 1, 12, 512)\n",
            "(373, 1024)\n",
            "output 373\n",
            "(373, 2)\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 0, 0, 1, 1, 0, 0, 1]\n",
            "373 416 330\n",
            "Para k: 9\n",
            "Acuracy tf.Tensor(0.73458445, shape=(), dtype=float32)\n",
            "False negative tf.Tensor(28.0, shape=(), dtype=float32)\n",
            "False positive tf.Tensor(71.0, shape=(), dtype=float32)\n",
            "True Negative tf.Tensor(89.0, shape=(), dtype=float32)\n",
            "True positive tf.Tensor(185.0, shape=(), dtype=float32)\n",
            "Presición tf.Tensor(0.72265625, shape=(), dtype=float32)\n",
            "recall tf.Tensor(0.8685446, shape=(), dtype=float32)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}